% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{amsfonts} % for maths fonts
\usepackage{amsmath} % for mathy text templates (propositions, theorems etc.)
\usepackage{amsthm} % proof env
\usepackage[hidelinks]{hyperref} % clickable, interactive references to equations, pages etc
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% Math templates
\newtheorem{prop}{Proposition}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}

%%% END Article customizations

%%% The "real" document content comes below...

\title{A formal introduction to RL theory}
\author{Sebastian Scherer}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle

\section{Why another introduction?}

The goal of this work is to provide a mathematically rigorous introduction to RL theory based on the excellent book "Introduction to reinforcement learning" by Sutton and Bartos (first edition). While I greatly enjoyed reading this book and appreciated its focused approach on developing an intuition for the Q- and V-functions, the algorithms and the general probabilistic framework introduced in the early chapters, I couldn't help but stumble at some points wondering how exactly a particular claim was justified. When I tried to bridge these gaps, further gaps unravelled, sometimes turning into chasms that I simply could not bridge using the theory presented in this book alone.  Queries on stack exchange as well as the various alternative resources applying even less rigour and, often times, introducing additional confusing notation, motivated me to  try and remedy this myself. I therefore set out to try and formalize the theory presented, at least for the finite Markov Decicions Processes treated in the book, so that it may help let my inner mathematician sleep at night, as well as, and this is my sincere hope, provide a rigorous and helpful introduction for all those who are not only interested in the intuition but also appreciate a firm foundation on which to place it. The following manuscript can be used as an explanatory guide to the concepts presented in the book, or can be independently used as a rigourous introduction to value function theory in its own right.

\section{Some notation}

Like the reference book, we consider finite state, finite action markov decision processes ("finite MDPs"). As such, we denote by $S$ the set of states achievable for a given finite MDP, and by $A$ the set of executable actions $a$. We do not restrict ourselves to deterministic polices, and therefore treat a policy $\pi$ as a conditional probability distribution over the executable action set $A$, conditioned on a given current state from $S$. In other words, 

\begin{equation}
	\begin{array}{llll}\label{ar_polMap}
		\pi 	& : A \times S 	& \to 	& [0,1] \\
			& (a,s)		& \mapsto	& \pi(a,s)
	\end{array}
\end{equation}

where $\pi(a,s) = Pr_{\pi}(a | s)$ denotes the probability of choosing action $a \in A$ when in state $s \in S$ while acting according to policy $\pi$.

We encode our knowledge about the (reactionary) nature of our environment via the transition probabilities

\begin{equation}\label{eq_transProb}
	P_{s,s'}^{a} = Pr(s_{t+1} = s' | s_t = s, a_t = a)
\end{equation}
 
where $s,s' \in S$ and $a \in A$, denoting the probability of ending up in state $s'$ at $t+1$ when coming from state $s$ at $t$ by executing $a$, and 

\begin{equation}\label{eq_rewExpVal}
	R_{s,s'}^{a} = \mathbb{E}[r_t | s_t = s, s_{t+1} = s', a_t = a]
\end{equation}

denoting the expected reward at time $t$ due to ending up in state $s'$ at $t+1$ when coming from state $s$ at $t$ by executing $a$. For the remainder of this script, we will assume a uniform bound on rewards throughout all time steps, i.e. that 

\begin{equation}\label{eq_rewBound}
	-M \le r_t \le M
\end{equation}

for some $M \in \mathbb{R}$ and each $t \in 0,1,2,\dots $.

Note that, in our notation, $s_t$ and $a_t$ denote the state and action at time $t$ respectively, and thus $r_t$ - NOT $r_{t+1}$ as in the book - denotes the reward obtained AFTER being in $s_t$ and executing $a_t$, thereby resulting in some (possibly the same) state $s_{t+1}$.

We use the same symbol $\gamma \in (0,1)$ to denote the reward discount factor.

Finally, the most difficult notation to right \textit{and} consistent: expected values. We will use slightly different notations to indicate the various different underlying distriutions that govern the behaviour of the random variables involved, and w.r.t which the expected value needs to be viewed.

If we are dealing with an implicit sequence of actions chosen according to one policy like 

\begin{equation}\label{eq_trajIm}
	s_t \overset{\pi}{\rightarrow} a_t \overset{P^{a_t}_{s_t,\cdot}}{\rightarrow} s_{t+1} \overset{\pi}{\rightarrow} a_{t+1} \overset{P^{a_{t+1}}_{s_{t+1},\cdot}}{\rightarrow} s_{t+1} \overset{\pi}{\rightarrow} \dots,
\end{equation}

we will express this by writing $\mathbb{E}_{\pi}[\cdot]$. The contribution of the environment's state distribution $P^{a_t}_{s_t.\cdot}$ is implicit since we usually deal with one finite MDP at a time, thereby keeping this particular distribution constant throughout all of the proofs. An example of this is $$\mathbb{E}_{\pi}[r_t | s_t = s],$$ which implies action $a_t$ was taken according to $\pi$ having started at state $s$ at time $t$.

Therefore, if the random variable in question is only dependent on the environment's distribution, such as in the expression $$\mathbb{E}[r_t + \gamma f(s_{t+1}) | s_t = s, a_t = a]$$ (where $f$ is some deterministic function) we omit any index. Note that in this case both the immediate reward $r_t$ as well as the next time step's state $s_{t+1}$ are entirely dependent on the environment parameters $R^a_{s,s'}$ and $P^a_{s,s'}$, since we fixed the action $a_t$, thereby cutting any potential policy out of the loop.

In some cases, we will need to indicate the involved random variables' distributions explicitly and individually. In those cases, we will make clear what exactly we mean.

\section{Value function and action-value function}

As Sutton and Bartos point out, the standard approach to the analysis of optimal behaviour w.r.t a given MDP is by closely examining the value function $V^{\pi}$ and action value function $Q^{\pi}$ associated to a given policy $\pi$. They are an essential tool for quantitative analysis of policy driven behaviour, and thus, unsurprisingly, we will make heavy use of them throughout this guide. For a given policy $\pi$ on a finite MDP, we call

\begin{equation}\label{ar_valFunc}
	\begin{array}{llll}
		V^{\pi} :	& S 	& \to 	& \mathbb{R} \\
				& s	& \mapsto	& \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{t+k} | s_t = s]
	\end{array}
\end{equation}

the \textit{value function} of $\pi$, and 

\begin{equation}\label{ar_actValFunc}
	\begin{array}{llll}
		Q^{\pi} : 	& A \times S 	& \to 	& \mathbb{R} \\
				& (a,s)		& \mapsto	& \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{t+k} | s_t = s, a_t = a]
	\end{array}
\end{equation}

the \textit{action-value} function of $\pi$. The general idea of $V^{\pi}$ is the quantification of the value a certain state $s$ possesses under $\pi$, by assigning it the expected cumulative reward obtained when starting in that state $s$ and then acting (i.e. choosing and exeucting actions) according to $\pi$. $Q^{\pi}$ does very much the same thing, except for pairs of states and actions $(a,s)$, assigning expected cumulative rewards when starting from $s$ via action $a$, and only \textit{then} acting according to $\pi$.

\subsection{Existential crisis}

In line with our rigorous approach so far, we dedicate this subsection to proving the well definedness of the above quantities. That is, we get assurances for:

\begin{itemize}
	\item the well definedness of the random variable defined by the series $G_t := \sum_{k=0}^{\infty}\gamma^k r_{t+k}$
	\item the well definedness of its expected value, i.e. the well definedness of $V^{\pi}$
	\item the well definedness of $Q^{\pi}$.
	\item some handy derivative results from the above three 
\end{itemize}

We start our quest with guaranteeing the well definedness of $G_t$. As a random variable that is the discounted infinite sum of random variables $r_{t+k}$, $k=0,1,\dots$, it maps sequences $\omega = (s_t, a_t, s_{t+1}, a_{t+1}, \dots ) $, with $s_t,s_{t+1},\dots S$, $a_t, a_{t+1},\dots A$ onto the set of real numbers $\mathbb{R}$. In other words, by denoting $\Omega$ as the set of all such sequences $\omega$, we consider $G_t$ to be

\begin{equation}
	\begin{array}{rlll}\label{ar_cumRew}
		G_t: 	& \Omega 		& \rightarrow 	& \mathbb{R} \\
			&	\omega	& \mapsto 	& \sum_{k=0}^{\infty} \gamma^k r_{t+k}(\omega)
	\end{array}
\end{equation}

where $\sum_{k=0}^{\infty} \gamma^k r_{t+k}(\omega) \overset{!}{\in} \mathbb{R}$ remains to be shown to guarantee well definedness of this quantity as a (random variable) mapping. We will do so in the following

\begin{prop}\label{prop_cumRewEx}{(Cumulative rewards exist)}
	The random variable $G_t$ as defined in \ref{ar_cumRew} is well defined. In other words, the series $G_t(\omega) = \sum_{k=0}^{\infty} \gamma^k r_{t+k}(\omega)$ is convergent for all $\omega \in \Omega$.
\end{prop}

\begin{proof}
We will show that for any trajectory $\omega \in \Omega$ the sequence $G_t^n(\omega) := \sum_{k=0}^{n} \gamma^k r_{t+k}(\omega) \in \mathbb{R}$ is a cauchy sequence. Since $\mathbb{R}$ is complete, this will imply convergence of $G_t^n(\omega)$ in $\mathbb{R}$.

Let w.l.o.g $n > m$ to see that

	\begin{equation}\label{eq_cumRew1}
		\left| G_t^n(\omega) - G_t^n(\omega)\right| = \left| \sum_{k=m}^n \gamma^k r_{t+k}(\omega) \right| \overset{\ref{eq_rewBound}}{\le} M \sum_{k=m}^n \gamma^k.
	\end{equation}

Since $s_n := \sum_{k=0}^n \gamma^k \overset{n \rightarrow \infty}{\rightarrow} \frac{1}{1-\gamma}$ and therefore cauchy, there exists for every $\hat{\epsilon}$ an $N_{\hat{\epsilon}}$ such that, if, $n,m \ge N_{\hat{\epsilon}}$, $\sum_{k=m}^n \gamma^k = |s_n - s_m| < \hat{\epsilon}$. If now $\epsilon >0$ is arbitrary but fixed, choose $\hat{\epsilon} = \frac{\epsilon}{M}$. Thus, for any $n,m > N_{\hat{\epsilon}}$, we have

\begin{equation}\label{eq_cumRew2}
	\left|G_t^n(\omega) - G_t^n(\omega)\right| \overset{\ref{eq_cumRew1}}{\le} M |s_n - s_m| < M \frac{\epsilon}{M} = \epsilon.
\end{equation}

Thus for any $\omega \in \Omega$, the sequence $G_t^n(\omega)$ is cauchy, and thus convergent on $\mathbb{R}$. This proves the claim.

\end{proof}

Having established $G_t(\omega) \in \mathbb{R}$, we can immediately see that it must be uniformly bounded on $\Omega$.

\begin{cor}{(Cumulative rewards are bounded)}\label{cor_unifBoundCumRew}
	The cumulative reward $G_t$ as defined in \ref{ar_cumRew} is uniformly bounded on $\Omega$. In particular, 

	\begin{equation}\label{eq_boundCumRew}
		|G_t(\omega)| < \frac{M}{1 - \gamma}
	\end{equation}

	for all $\omega \in \Omega$.
\end{cor}

\begin{proof}
	Thanks to Prop \ref{prop_cumRewEx} we can justify writing

\begin{equation}
	|G_t(\omega)| = \left|\sum_{k=0}^{\infty} \gamma^k r_{t+k}(\omega) \right| \le \sum_{k=0}^{\infty} \gamma^k |r_{t+k}(\omega)| \overset{\ref{eq_rewBound}}{=} M \sum_{k=0}^{\infty} \gamma^k = \frac{M}{1 - \gamma}.
\end{equation}

\end{proof}

Now that the infinite discounted reward is a well defined random variable, we can sensibly pose the question regarding its expected value. Before we do so, however, let us have a look at the trajectory space $\Omega$ and consider why it is a countable, discrete set.

Let us denote by $c_S = |S|, c_A = |A|$ the cardinality, i.e. size, of the state space $S$ and action space $A$, respectively. We can then order all possible trajectories in 

\begin{equation}\label{def_traj}
	\Omega = \{ \omega = (s_1, a_2, s_2, a_2, \dots) | s_i \in S, a_i \in A, i=1,2,\dots \}.
\end{equation}

To see such an ordering, refer to the appendix (? - still have to write this part). In particular, whenever we write $\omega_i$ for $i = 1,2,\dots$ or similar we mean the ordered $\omega \in \Omega$.

We now establish the existence of the expected value of $G_t$.

\begin{lem}\label{lem_expCumRew}{(Expected cumulative rewards exist, too)}
	Let $P_\Omega$ be any trajectory distribution on the trajectory space $\Omega$. Then the expected value of the cumulative reward $G_t$ under that distribution is well defined and exists. In other words $\mathbb{E}_{\omega \sim P_{\Omega}}[G_t] < \infty$.
\end{lem}

Note that this is a pretty general statement about sufficient conditions for the existence of $G_t$'s expected value. It includes, for example, the cases were we act accoding to some policy $\pi$, starting from some random state $s$ distributed according to some random distribution $P_S$ on $S$. In particular, if that distribution is deterministic, i.e. if 

\begin{equation}\label{eq_detStateDist}
	P_{S,s'}(s) = 	\begin{cases}
				1 & \text{ for } s = s' \\
				0 & \text{ else}
			\end{cases}
\end{equation}

we will have established the well-definedness of $V_{\pi}$ as defined in Definition \ref{ar_valFunc}, since that is what the value function, according to \ref{ar_valFunc}, measures. To see this, note that in such a case, $P_{\Omega} = P_{\Omega | s'}^{\pi} = P^{\pi} \circ P_{S,s'}$ is the distribution resulting from sampling a starting state $s$ according to $P_{S,s'}$, and then, given such a starting state, sampling a trajectory $\omega$ according to $\pi$. In such a case we have

\begin{equation}\label{eq_finiteValFunc}
	V^{\pi}(s') \overset{Def \ref{ar_valFunc}}{:=} \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k} | s_t = s\right] = \mathbb{E}_{P_\Omega}[G_t] \overset{Lem \ref{lem_expCumRew}}{<} \infty
\end{equation}

The analogous argument guaranteeing the well definedness of our action-value function $Q^{\pi}$ as defined in Definition \ref{ar_actValFunc}, uses a deterministic distribution $P_{S \times A, (s',a')}$ such that, for a fixed but arbitrarty pair $(s',a') \in S \times A$, 

\begin{equation}\label{eq_detActStateDist}
	P_{S \times A,(s',a')}(s,a) = 	\begin{cases}
				1 & \text{ for } (s,a) = (s',a') \\
				0 & \text{ else}.
			\end{cases}
\end{equation}

This leads to a composite trajectory distribution $P_{\Omega} = P_{\Omega | s',a'}^{\pi} = P^{\pi} \circ P_{S,(s',a')}$ resulting from sampling a starting state \textit{and} action $(s,a)$ according to $P_{S,(s',a')}$, and then, given such a starting state-action pair, sampling a trajectory $\omega$ according to $\pi$. This yields

\begin{equation}\label{eq_finiteActValFunc}
	Q^{\pi}(s',a') \overset{Def \ref{ar_actValFunc}}{:= }\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^k r_{t+k} | s_t = s', a_t = a'\right] = \mathbb{E}_{P_\Omega}[G_t] \overset{Lem \ref{lem_expCumRew}}{<} \infty.
\end{equation}

For easier reference later on, let's summarize these arguments in the form of the following 

\begin{cor}{((Action-)Value functions exist)}
	Let $\pi$ be probabilistic or deterministic policy on a finite MDP. Then the functions $V^{\pi}$ as defined in \ref{ar_valFunc} and $Q^{\pi}$ as defined in \ref{ar_actValFunc} are well defined.
\end{cor}

\begin{proof}
	For a proof of both claims, see the argumentats presented in the paragraph preceding this corollary. In particular, for the claim about $V^{\pi}$ see the derivation of equations \ref{eq_detStateDist} and \ref{eq_finiteValFunc}. Similarly, the well definedness of $Q^{\pi}$ is justified by the arguments formalized in equations \ref{eq_detActStateDist} and \ref{eq_finiteActValFunc}.
\end{proof}

Having established the usefulness of this Lemma, let us finally prove it.

\begin{proof}{(of Lemma \ref{lem_expCumRew})}
	Given some trajectory distribution $P_{\Omega}$ on the (unordered) set $\Omega$, we need to show convergence of the series 

\begin{equation}\label{eq_showConv}
	\mathbb{E}_{\omega \sim P_{\Omega}}[G_t] = \sum_{\omega \in \Omega} P_{\Omega}({\omega}) G_t(\omega) < \infty
\end{equation}

\textit{regardless} of the particular ordering of the trajectories $\omega$. We do this by showing the convergence of this series' absolute series counterpart, namely

\begin{equation}\label{eq_showAbsConv}
	\sum_{\omega \in \Omega} | P_{\Omega}({\omega}) G_t(\omega) | \overset{P_\Omega(\omega) \ge 0}{=} \sum_{\omega \in \Omega} P_{\Omega}({\omega}) | G_t(\omega) | = \sum_{i=1}^{\infty} P_{\Omega}({\omega_i}) | G_t(\omega_i) | < \infty
\end{equation}

for the \textit{one particular ordering} of $\Omega$ referred to earlier. The convergence of the absolute series in \ref{eq_showAbsConv} for one possible ordering of $\omega \in \Omega$ then guarantees the convergence of the series in $\ref{eq_showConv}$ for \textit{any} ordering of $\omega \in \Omega$ to the same limit according to the "Famous Reordering Series Convergence Theorem". To show that the absolute series converges, we show that the sequence

\begin{equation}\label{eq_partSeq}
	EG_{abs}^n := \sum_{i=1}^n |G_t(\omega_i)| P_\Omega(\omega_i) \overset{Prop \ref{prop_cumRewEx}}{<} \infty
\end{equation}

induced by its partial sum is a Cauchy sequence. W.lo.g, let $n>m$ to see that

\begin{equation}\label{ar_cauchySeq}
	\begin{array}{rcl}
	\left| EG_{abs}^n - EG_{abs}^m \right| 	& = & \sum_{i=m}^n |G_t(\omega_i)| P_\Omega(\omega_i) \\ 
									& \overset{Cor \ref{cor_unifBoundCumRew}}{\le} & \frac{M}{1 - \gamma} \sum_{i=m}^n  P_{\Omega}(\omega_i). \\
	\end{array}
\end{equation}

Since $\sum_{i=0}^{\infty} P_{\Omega}(\omega_i) = 1$, the sequence of partial sums

\begin{equation}
	\hat{s}_n := \sum_{i=0}^n P_{\Omega}(\omega_i)
\end{equation}

is necessarily convergent and thus a Cauchy sequence. This means that for any $\hat{\epsilon} > 0$, we can find an $N_{\hat{\epsilon}} \in \mathbb{N}$ such that for all $n>m>N_{\hat{\epsilon}}$ we have 

\begin{equation}\label{eq_partProbSeq}
	\sum_{i=m}^n  P_{\Omega}(\omega_i) \overset{P_{\Omega}(\omega_i) \ge 0}{=} |\hat{s}_n - \hat{s}_m| < \hat{\epsilon}.
\end{equation}

Let $\epsilon >0$ be arbitrary but fixed. Choosing $\hat{\epsilon} = \epsilon \frac{1 - \gamma}{M}$ and $n>m>N_{\hat{\epsilon}}$, we indeed see that

\begin{equation}
	| EG_{abs}^n - EG_{abs}^m | \overset{\ref{ar_cauchySeq}}{\le} \frac{M}{1- \gamma} \sum_{i=m}^n P_{\Omega}(\omega_i) \overset{\ref{eq_partProbSeq}}{\le} \frac{M}{1 - \gamma} \hat{\epsilon} = \epsilon,
\end{equation}

proving that $(EG_{abs}^n)_n$ is indeed Cauchy and thus convergent in $\mathbb{R}$. This proves \ref{eq_showAbsConv} for the one ordering of $\Omega$ constructed, and thus \ref{eq_showConv} for \textit{any} ordering of $\omega \in \Omega$. This proves the Lemma.

\end{proof}

In a similar vein, the expectations of the individual discounted rewards also exist for any finite MDP.

\begin{lem}\label{lem_expIndRew}
	Let $k \in \mathbb{N}$ be fixed but arbitrary. Then the expectation of the discounted reward at time step $t+k$, where $t$ is the starting point in time for all trajectories considered, exist and are bounded by $M$. The same holds for the absolute value of the discounted reward at time step $t+k$. In other words, we have both $\mathbb{E}_{\omega \sim P_\Omega}[\gamma^k r_{t+k}] \le \gamma^k M$ and $\mathbb{E}_{\omega \sim P_\Omega}[|\gamma^k r_{t+k}|] \le \gamma^k M$.
\end{lem}

\begin{proof}
	The proof for either claim is nearly identical to arguments presented in the proof of Lemma \ref{lem_expCumRew}. To avoid repetition we point out the two differences: instead of $G_t(\omega)$, we use either $\gamma^k r_{t+k}$ or $|\gamma^k r_{t+k}| = \gamma^k |r_{t+k}|$ depending on which of the two claims we want to prove. Secondly, the inequality in \ref{ar_cauchySeq} is now justified by the assumption of universal boundedness \ref{eq_rewBound}, and the bound $\frac{M}{1 - \gamma}$ is to be replaced by $\gamma^k M$. Virtually every other argument can be copy pasted to prove both $\mathbb{E}_{\omega \sim P_\Omega}[\gamma^k r_{t+k}] \le \gamma^k M$ and $\mathbb{E}_{\omega \sim P_\Omega}[|\gamma^k r_{t+k}|] \le \gamma^k M$.
\end{proof}

Before concluding this existential episode, let us present one more result and its application to our value function setting.

\begin{lem}{(Infinite linearity)}\label{lem_limExpSwitch}
	Let $P_\Omega$ be a distribution on the discrete trajectory space $\Omega$. Then
	\begin{equation}\label{eq_infLinGt}
		\mathbb{E}_{\omega \sim P_\Omega}\left[ G_t(\omega) \right] = \lim_{n \rightarrow \infty} \mathbb{E}_{\omega \sim P_\Omega}\left[ \sum_{k=0}^n \gamma^k r_{t+k} \right]
	\end{equation}
\end{lem}

\begin{proof}
	The first step of the proof is to reduce the claim to that of linearity of the expectation for infinite random series, i.e. to reduce our claim to 
	\begin{equation}\label{eq_infLinX}
		\mathbb{E}_{\omega \sim P_\Omega}\left[ \sum_{k=0}^{\infty} X_k \right] \overset{!}{=} \sum_{k=0}^{\infty} \mathbb{E}_{\omega \sim P_\Omega}\left[ X_k \right]
	\end{equation}

	and then refer to a standard argument in probability theory. To see that claim \ref{eq_infLinGt} can be rephrased in the form of \ref{eq_infLinX}, set $X_k := \gamma^k r_{t+k}$ for $k=0,1,\dots$, giving equality of the respective left hand sides. Further, note that with these $X_k$ and the fact that expectations are \textit{always} linear for \textit{finite} sums of random variables, we can see that the right hand side of \ref{eq_infLinGt} can be rewritten like

	\begin{equation}
		\begin{array}{rcl}
		\lim_{n \rightarrow \infty} \mathbb{E}_{\omega \sim P_\Omega}\left[ \sum_{k=0}^n \gamma^k r_{t+k} \right] & = & \lim_{n \rightarrow \infty} \mathbb{E}_{\omega \sim P_\Omega}\left[ \sum_{k=0}^n X_k \right] \\
			& = & \lim_{n \rightarrow \infty} \sum_{k=0}^n \mathbb{E}_{\omega \sim P_\Omega}\left[  X_k \right] \\
			& = & \sum_{k=0}^{\infty} \mathbb{E}_{\omega \sim P_\Omega}\left[ X_k \right],
		\end{array}
	\end{equation}

	showing the claimed equivalence of both statements. To prove our modified claim \ref{eq_infLinX}, a standard probability theory argument states that all we require is for 
	\begin{equation}
		\sum_{k=0}^{\infty} \mathbb{E}_{\omega \sim P_\Omega}\left[ | X_i | \right] \overset{!}{<} \infty
	\end{equation}

	to hold. This of course is equivalent to showing 
	\begin{equation}\label{eq_finiteInfDiscExp}
		\sum_{k=0}^{\infty} \mathbb{E}_{\omega \sim P_\Omega}\left[ \gamma^k | r_{t+k} | \right] \overset{!}{<} \infty,
	\end{equation}

	where we already exploited $\gamma > 0$. We can quickly verify that indeed

	\begin{equation}
		\begin{array}{rcl}
		\sum_{k=0}^{n} \mathbb{E}_{\omega \sim P_\Omega}\left[ \gamma^k | r_{t+k} | \right] & \overset{Lem \ref{lem_expIndRew}}{\le} & \sum_{k=0}^{n} \gamma^k M \\
			& < & \sum_{k=0}^{\infty} \gamma^k M \\
			& = & \frac{M}{1 - \gamma}.
		\end{array}
	\end{equation}

	Letting $n \rightarrow \infty$ on either side, we clearly have \ref{eq_finiteInfDiscExp}, thus proving the required condition and therefore proving the original claim \ref{eq_infLinGt}.
\end{proof}

A useful consequence of this result is an alternative expression for our (action-) value functions $V^{\pi}$ and $Q^{\pi}$ for some policy $\pi$ on a finite MDP.

\begin{cor}{(Limit your expectations)}\label{cor_valSwitch}
	Let $\pi$ be a policy on a finite MDP. Then
	\begin{enumerate}
		\item \begin{equation}\label{eq_valSwitch} V^{\pi}(s) = \lim_{n \rightarrow \infty} \mathbb{E}_{\pi}\left[ \sum_{k=0}^n \gamma^k r_{t+k} | s_t = s \right] \end{equation}
		\item \begin{equation}\label{eq_actValSwitch} Q^{\pi}(s,a) = \lim_{n \rightarrow \infty} \mathbb{E}_{\pi}\left[ \sum_{k=0}^n \gamma^k r_{t+k} | s_t = s, a_t = a \right] \end{equation}
	\end{enumerate}
\end{cor}

\begin{proof}
	The proof for either identity goes all the way back (see the section surrounding equations \ref{eq_detStateDist}, \ref{eq_finiteValFunc}, \ref{eq_detActStateDist} and \ref{eq_finiteActValFunc}) to the realisation that fixing the starting state $s_t$  and then following a given policy $\pi$ is inducing a distribution $P_{\Omega} = P_{\Omega | s'}^{\pi} = P^{\pi} \circ P_{S,s'}$ on the trajectory space $\Omega$. We can write

	\begin{equation}
		\begin{array}{rcl}
		V^{\pi}(s) & = & \mathbb{E}_{\pi}\left[ G_t | s_t = s \right] \\
				& = & \mathbb{E}_{\omega \sim P_\Omega}\left[ G_t \right] \\
				& \overset{Lem \ref{lem_limExpSwitch}}{=} & \lim_{n \rightarrow \infty} \mathbb{E}_{\omega \sim P_\Omega}\left[ \sum_{k=0}^n \gamma^k r_{t+k} \right] \\
				& = & \lim_{n \rightarrow \infty} \mathbb{E}_{\pi}\left[ \sum_{k=0}^n \gamma^k r_{t+k} | s_t = s \right],
		\end{array}
	\end{equation}

	proving \ref{eq_valSwitch}. Equivalently, fixing a starting state $s_t$ \textit{and} starting action $a_t$ and then following a given policy $\pi$ induces a distribution $P_{\Omega} = P_{\Omega | s',a'}^{\pi} = P^{\pi} \circ P_{S,(s',a')}$. This allows us to write

	\begin{equation}
		\begin{array}{rcl}
		Q^{\pi}(s,a) & = & \mathbb{E}_{\pi}\left[ G_t | s_t = s, a_t = a \right] \\
				& = & \mathbb{E}_{\omega \sim P_\Omega}\left[ G_t \right] \\
				& \overset{Lem \ref{lem_limExpSwitch}}{=} & \lim_{n \rightarrow \infty} \mathbb{E}_{\omega \sim P_\Omega}\left[ \sum_{k=0}^n \gamma^k r_{t+k} \right] \\
				& = & \lim_{n \rightarrow \infty} \mathbb{E}_{\pi}\left[ \sum_{k=0}^n \gamma^k r_{t+k} | s_t = s, a_t = a \right],
		\end{array}
	\end{equation}

	proving \ref{eq_actValSwitch}.

\end{proof}

\subsection{Relationships}

This subsection is dedicated to highlighting important and useful relationships between value and action-value functions. We will have a look at expressing one via the other, as well as recursive identities across time steps $t$. 

Due to the heavy lifting done in the previous subsection as well as the inherently local (w.r.t time $t$, that is) behaviour of (finite) MDP, all probabilistic considerations in this section reduce to finite event spaces. In particular, instead of dealing with infinite but discrete trajectories $\omega = (s_t,a_t,s_{t+1},a_{t+1},\dots)$, we can focus on a finite set of state or action random variables $(s_t,a_t),\dots,(s_{t+k},a_{t+k})$, where $k$ will depend on the given context, but most of the times will not exceed 1. All expected values are therefore to be understood with respect to the finite distributions on these finite state-action space tuples  resulting from following some specified policy $\pi$, sometimes with additional constraints on starting states or actions $s_t, a_t$. While the underlying trajectory distributions $P_\Omega$ as outlined in the section surrounding equations \ref{}, \ref{}, \ref{} and \ref{} technically still apply, the properties defining a (finite) MDP as well as our previous work allow us to break free from this slightly technical "underground" world and instead work in the "above-ground" and easy-to-handle environment of finite state-action distributions. 

The connection between these two worlds, in particular the transformation from trajectories $\omega$ to individual states $s_{t+k}$ and actions $a_{t+k}$, can be done by effectively grouping trajectories' probabilities that share, say, a state $s$ at a fixed point in time $t+k$. To be precise, for some distribution $P_\Omega$ on $\Omega$, we can generate a distributions $P_{P_\Omega,S}$ for $s_{t+k}$ on $S$ by putting

\begin{equation}
	P_{P_\Omega,S}(s_{t+k} = s) := \sum_{\omega \in \Omega^{t+k}_s} P_\Omega(\omega),
\end{equation}

where $\Omega^{t+k}_s := \{\omega = (s_t,a_t,s_{t+1},a_{t+1},\dots) \in \Omega | s_{t+k} = s\}$. Similar constructions can be made to generate distributions over the action space $A$ for the random variable $a_{t+k}$ representing the action chosen at time $t+k$, and so on. In this sense, any policy $\pi$ (with optional constraints on states and/or actions at some fixed point(s) in time) generates some $P_\Omega$ on $\Omega$, which in turn generates a host of distributions on $S$ and $A$, for individual states and actions, respectively. It is in this way that the following results can be connected to the results in the previous section. Personally, I found it helpful to keep in mind the concept of an "original source" distribution $P_\Omega$ when dealing with different policies' behaviour and relative performance.

Having said all that, let us have a closer look at the recursive nature of both $V^{\pi}$ and $Q^{\pi}$ - a feature we will exploit throughout the rest of this transcript.

\begin{prop}{(Parametrized bellman equations)}
	Let $\pi$ be an arbitrary policy, and let $V^{\pi}$ and $Q^{\pi}$ its associated (action-)value functions. Then the following identities hold:
	\begin{enumerate}
		\item $V^{\pi}(s) = \sum_{a} \pi(s,a) \sum_{s'} P_{s,s'}^a ( R_{s,s'}^a + \gamma V^{\pi}(s') ) $
		\item $Q^{\pi}(s,a) = \sum P_{s,s'}^a [ R_{s,s'}^a + \gamma \sum_{a'} \pi(s',a') Q^{\pi}(s',a') ] $
	\end{enumerate}
\end{prop}

\begin{proof}
	To prove the first identity consider
	\[
		\begin{array}{rl}
			V^{\pi}(s)		& = \mathbb{E}_{\pi}[ \sum_{k=0}^{\infty} \gamma^k r_{t+k} | s_t = s]  \\
						& = \mathbb{E}_{\pi}[r_t + \gamma \sum_{k=0}^{\infty} \gamma^k r_{t+1+k} | s_t = s]  \\
						& = \mathbb{E}_{\pi}[r_t | s_t = s] + \mathbb{E}_{\pi}[\gamma \sum_{k=0}^{\infty} \gamma^k r_{t+1+k} | s_t = s] \\
						& \overset{\ref{eq_transProb},\ref{eq_rewExpVal}}{=} \sum_a \pi(s,a) \sum_{s'} P_{s,s'}^a R_{s,s'}^a + \gamma \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{t+1+k} | s_t = s] \\
						& = \sum_a \pi(s,a) \sum_{s'} P_{s,s'}^a R_{s,s'}^a + \gamma \sum_a \pi(s,a) \sum_{s'} P_{s,s'}^a \mathbb{E}_{\pi} [\sum_{k=0}^{\infty} \gamma^k r_{t+1+k} | s_{t+1} = s'] \\
						& = \sum_a \pi(s,a) \sum_{s'} P_{s,s'}^a (R_{s,s'}^a + \gamma \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{t'+k} | s_{t'} = s'] ) \\
						& = \sum_a \pi(s,a) \sum_{s'} P_{s,s'}^a (R_{s,s'}^a + \gamma V^{\pi}(s') ).
		\end{array}
	\]
	Note how we used the transitional probabilities and expected rewards to explicitly write out the expected value of the cumulative reward $\sum_{k=0}		^{\infty} \gamma^k r_{t+k}$ established in Lemma \ref{lem_expCumRew}. Note also how we wrote $\mathbb{E}_{\pi}$ to indicate that the expected value is w.r.t to a sequence of actions $a_t, a_{t+1},\dots$ and states $s_t (=s$, as per explicit condition $"| s_t = s"),s_{t+1},\dots$ resulting from acting according to $\pi$, made explicit in subsequent steps including the term $\pi(s,a)$.

A similar line of thinking shows us that
	\[
		\begin{array}{rl}
			Q^{\pi}(s,a)	& = \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{t+k} | s_t = s, a_t = a]  \\
						& = \mathbb{E}_{\pi}[r_t + \gamma \sum_{k=0}^{\infty} \gamma^k r_{t+1+k} | s_t = s, a_t = a]  \\
						& = \mathbb{E}_{\pi}[r_t | s_t = s, a_t = a] + \gamma \mathbb{E}_{\pi}[ \sum_{k=0}^{\infty} \gamma^k r_{t+1+k} | s_t = s, a_t = a] \\
						& \overset{\ref{eq_transProb},\ref{eq_rewExpVal}}{=} \sum_{s'} P_{s,s'}^a R_{s,s'}^a \\
						& + \gamma \sum_{s'} P_{s,s'}^a \sum_{a'} \pi(s,a') \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{t+1+k} | s_t = s, a_t = a, s_{t+1} = s', a_{t+1} = a'] \\
						& = \sum_{s'} P_{s,s'}^a R_{s,s'}^a + \gamma \sum_{s'} P_{s,s'}^a \sum_{a'} \pi(s,a') \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{t+1+k} | s_{t+1} = s', a_{t+1} = a'] \\
						& = \sum_{s'} P_{s,s'}^a R_{s,s'}^a + \gamma \sum_{s'} P_{s,s'}^a \sum_{a'} \pi(s,a') \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{t'+k} | s_{t'} = s', a_{t'} = a'] \\
						& = \sum_{s'} P_{s,s'}^a R_{s,s'}^a + \gamma \sum_{s'} P_{s,s'}^a \sum_{a'} \pi(s,a') Q^{\pi}(s',a') \\
						& = \sum_{s'} P_{s,s'}^a ( R_{s,s'}^a + \gamma \sum_{a'} \pi(s,a') Q^{\pi}(s',a') )  \\
		\end{array}
	\]

Note that we used the markov property which allowed us to drop past states and actions when going from line 4 to line 5.

\end{proof}

Remembering the definitions of $P_{s,s'}^a$ and $R_{s,s'}^a$, these parametrizations can be rewritten in a slightly more compact way:

$$ V^{\pi}(s) =  \mathbb{E}_{\pi}[r_t + \gamma V^{\pi}(s_{t+1}) | s_t = s ] $$

and 

$$ Q^{\pi}(s) = \mathbb{E}_{\pi}[r_t + \gamma Q^{\pi}(s_{t+1},a_{t+1}) | s_t = s, a_t = a]. $$

These are the 'standard' bellman equations.

We now characterize the relationship between these two functions in the following

\begin{prop}{(QV Relationships)}\label{prop_qVRelShips}
	Let $\pi$ be an arbitrary policy. Then the following identities hold:
	\begin{enumerate}
		\item $V^{\pi}(s) = \sum_a \pi(s,a) Q^{\pi}(s,a)$ \\
		\item $V^{\pi}(s) = \sum_{a,s'}  \pi(s,a) P_{s,s'}^a [ R_{s,s'}^a + \pi(s',a') \gamma Q^{\pi}(s',a')] $ \\
		\item $Q^{\pi}(s,a) = \sum_{s'} P_{s,s'}^a [ R_{s,s'}^a + \gamma V^{\pi}(s') ] $
	\end{enumerate}
\end{prop}

\begin{proof}
	To see that the first claims holds, we use the explicit distribution of taking an action $a$ when in state $s$ and following $\pi$ to see that indeed
	\[
		\begin{array}{rl}
			V^{\pi}(s)		& =  \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{t+k} | s_t = s] \\
						& =  \sum_a \pi(s,a) \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{t+k} | s_t = s, a_t = a] \\
						& = \sum_a \pi(s,a) Q^{\pi}(s,a).
		\end{array}
	\]

	For the second claim, following a similar line of argument as we have done for Proposition 1, we see that

	\[
		\begin{array}{rl}
			V^{\pi}(s)		& = \mathbb{E}_{\pi}[r_t | s_t = s] + \gamma \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{t+1+k} | s_t = s] \\
						& = \sum_a \sum_{s'} \pi(s,a) P_{s,s'}^a R_{s,s'}^a \\
						& + \sum_a \sum_{s'} \pi(s,a) P_{s,s'}^a \gamma  \sum_{a'} \pi(s,a') \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{t+1+k} | s_t = s, s_{t+1} = s', a_{t+1} = a'] \\
						& = \sum_a \sum_{s'} \pi(s,a) P_{s,s'}^a R_{s,s'}^a \\
						& + \sum_a \sum_{s'} \pi(s,a) P_{s,s'}^a \gamma  \sum_{a'} \pi(s,a') \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{t'+k} | s_{t'} = s', a_{t'} = a'] \\
						& =  \sum_a \sum_{s'} \pi(s,a) P_{s,s'}^a ( R_{s,s'}^a + \gamma \sum_{a'} \pi(s',a') Q^{\pi}(s',a') ).
		\end{array}
	\]

	The third equality uses the markov property. Lastly, we verify that

	\[
		\begin{array}{rl}
			Q^{\pi}(s,a)  	& = \mathbb{E}[r_t | s_t = s, a_t = a] + \gamma \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{t+1+k} | s_t = s, a_t = a] \\
						& = \sum_{s'} P_{s,s'}^a R_{s,s'}^a + \gamma \sum_{s'} P_{s,s'}^a \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{r+1+k} | s_t = s, a_t = a, s_{t+1} = s'] \\
						& = \sum_{s'} P_{s,s'}^a R_{s,s'}^a + \gamma \sum_{s'} P_{s,s'}^a \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k r_{r'+k} | s_t' = s'] \\
						& = \sum_{s'} P_{s,s'}^a ( R_{s,s'} + \gamma V^{\pi}(s') )
		\end{array}
	\]

	completing the proof.

\end{proof}

As before we give the more compact versions of these identities:

$$ V^{\pi}(s) =  \mathbb{E}_{\pi}[Q^{\pi}(s_{t},a_{t}) | s_t = s ], $$
$$ V^{\pi}(s) =  \mathbb{E}_{\pi}[r_t + \gamma Q^{\pi}(s_{t+1},a_{t+1}) | s_t = s ] $$ and 
$$ Q^{\pi}(s,a) = \mathbb{E}[r_t + \gamma V^{\pi}(s_{t+1}) | s_t = s, a_t = a]. $$

Note that the last identity's expected value is \textit{not} w.r.t $\pi$ - that's because there is nothing random left to be determined according to $\pi$. The state $s_t$ is given, the action $a_t$ specified and fixed, and the expected value of the next state $s_{t+1}$ is entirely dependent on how the environment reacts to this combination; and the term $V^{\pi}$ is a deterministic function. This implies that policies sharing the same $V$ also share the same $Q$. The reverse is not necessarily true. We formalize this realisation in the subsequent

\begin{cor}\label{cor_sameVSameQ}
	Let $\pi_1,\pi_2$ be two arbitrary policies such that $V^{\pi_1} \equiv V^{\pi_2}$. Then $Q^{\pi_1} \equiv Q^{\pi_2}$
\end{cor}

\begin{proof}
	This is most easily seen in the original, parametrized formulation of Proposition 2, 3. . Since both $R_{s,s'}$ and $P_{s,s'}^a$ are dependent on the environment only, and not on the policy in question, we clearly have
	\[
		\begin{array}{rll}
			Q^{\pi_1}(s,a) & = &  \sum_{s'} P_{s,s'}^a [ R_{s,s'}^a + \gamma V^{\pi_1}(s') ] \\
						& = & \sum_{s'} P_{s,s'}^a [ R_{s,s'}^a + \gamma V^{\pi_2}(s') ] \\
						& = & Q^{\pi_2}(s,a)
		\end{array}
	\]
	for all $(s,a) \in S \times A$.
\end{proof}

Another useful result derived from the same identity is formalized in the below

\begin{cor}
	Let $\pi$ be a policy for a finite MDP, and let $(s,a) \sim P_{s,a}$ be a randomly distributed state-action pair. Then $$\mathbb{E}_{s, a \sim P_{s,a}}[Q^{\pi}(s,a)] = \mathbb{E}_{s_t, a_t \sim P_{s,a}}[r_t + \gamma V^{\pi}(s_{t+1})].$$
\end{cor}

\begin{proof}
	Since we are dealing with a finite MDP, both states and actions are drawn from a finite set $S$ and $A$, respectively. We can therefore write
	\[
		\begin{array}{rll}
			\mathbb{E}_{s,a \sim  P_{s,a}}[Q^{\pi}(s,a)] 	& = & \sum_{s,a} P_{s,a} Q^{\pi}(s,a) \\
				& = & \sum_{s,a} P_{s,a} \mathbb{E}[r_t + \gamma V^{\pi}(s_{t+1}) | s_t = s, a_t = a] \\
				& = & \mathbb{E}_{s_t,a_t \sim P_{s,a}}[r_t + \gamma V^{\pi}(s_{t+1})].
		\end{array}
	\]
\end{proof}

\subsection{Comparing and improving}

Now that we are a bit more comfortable with the concept of an (action-) value function, we can use it as a tool to quantify the \textit{quality} of a given policy. Intuitively, it makes sense to regard a policy $\pi_1$ that induces a higher expected reward when starting from a given state $s$ than, say, another policy $\pi_2$ as 'better'  - at least for that given state. In other words, it makes sense to regard $\pi_1$ as a better policy when starting from $s$ than $\pi_2$, if and only if $V^{\pi_1}(s) > V^{\pi_2}(s)$. Expanding this intuitive measure of comparison beyond a single state $s$ to \textit{all} elements of $S$, we arrive at the following natural

\begin{defn}{(Policy ranking)}
Let $\pi_1, \pi_2$ be policies for a finite MDP. We say that $$\pi_1 \ge_V \pi_2$$ if and only if $V^{\pi_1}(s) \ge V^{\pi_2}(s)$ for all $s \in S$. We say that $$\pi_1 >_V \pi_2$$ if and only if  $\pi_1 \ge_V \pi_2$ and there exists at least one $s \in S$ such that $V^{\pi_1}(s) > V^{\pi_2}(s).$ Finally, we say that $$\pi_1 =_V \pi_2$$ if and only if $V^{\pi_u}(s) = V^{\pi_l}(s)$ for all $s \in S$.
\end{defn}

This ranking of policies w.r.t $\ge_V$ induces a partial ordering on the set of policies $\Pi$. Note that it is possible that neither $\pi_1 \ge_V \pi_2$ nor $\pi_1 \le_V \pi_2$ for a given pair of policies $\pi_1,\pi_2$, since we demand that one value function exceeds the other for \textit{all} $s \in S$. In other words, $\ge_V$ really only is a \textit{partial} ordering on the set of policies $\Pi$.

We sometimes informally refer to the relation $\pi_1 \ge_V \pi_2$ as '$\pi$ is \textit{at least as good} as $\pi_2$', to $\pi_1 >_V \pi_2$ as '$\pi_1$ is \textit{better than / an actual improvement over} $\pi_2$' and to $\pi_1 =_V \pi_2$ as '$\pi_1$ is \textit{as good as / equally good as} $\pi_2$'.

We have, even at this early stage, established enough theory to characterize some cases where a direct comparison of policies w.r.t $\ge_V$ is possible.

\begin{thm}{(Policy improvement theorem)}
	Let $\pi_u, \pi_l$ be two different policies for a finite MDP such that $$ \mathbb{E}_{a \sim \pi_u(s,\cdot )}[Q^{\pi_l}(s,a)] \ge V^{\pi_l}(s)$$ for all $s \in S$. Then $$\pi_u \ge_V \pi_l.$$
\end{thm}

Before we begin the proof, let us formulate the above statement in a slightly less formal way. Our condition on $\pi_u$ and $\pi_l$ can be paraphrased as follows: If the expected reward generated by following $\pi_u$ for \textit{one} time step (note the expected value is indexed with $\pi_u$, indicating that the one remaining free random variable $a_t$ is chosen according to $\pi_u$ \textit{conditioned}, i.e. fixed in its state variable, on the value of the state $s$) and then following $\pi_l$ for all subsequent time steps is \textit{always} (i.e. for every starting state $s$) greater or equal to the expected reward generated by following $\pi_l$ \textit{from the start}, then the policy $\pi_u$ must be at least as good as $\pi_l$ overall. In other words, if 'prepending' your actions with one action from a specified policy does not deteriorate rewards, the policy generating that one inserted action at the start of your journeys is at least as good as the policy being prepended. We will actually use this idea in an induction approach to show that, as we iteratively increase the number of time steps in which the actions are being chosen according to $\pi_u$ before switching back to $\pi_l$, the expected reward does not decrease as well as converges to $V^{\pi_u}$.

Another thing to note is that, if the policy $\pi_u$ is deterministic, our condition in the theorem reduces to $$Q^{\pi_l}(s,\pi_u(s)) \ge V^{\pi_l}(s)$$ as the expected value of a constant random variable reduces to that constant value.

\begin{proof}
	We first need to extend our assumption to the case where the state $s$ appearing on both sides is not fixed, but more generally a random variable distributed according to, say, some distribution $P_s$. Since $P_s$ is a distribution over finite states, we can see that indeed
	\[
		\begin{array}{rll}
			\mathbb{E}_{s \sim P_s}[V^{\pi_l}(s)] & = & \sum_s P_s V^{\pi_l}(s) \\
				& \le & \sum_s P_s \mathbb{E}_{a \sim \pi_u(s,\cdot)}[Q^{\pi_l}(s,a)] \\
				& = & \sum_s P_s \sum_a \pi_u(s,a) Q^{\pi_l}(s,a) \\
				& = & \mathbb{E}_{s_t ,a_t \sim (P_{s_t},\pi_u(s_t,\cdot))}[Q^{\pi_l}(s_t,a_t)].
		\end{array}
	\]

	The second and main part of the proof will consist of showing the aforementioned policy improvement via prepending $\pi_l$ with $\pi_u$. Formally, this means that for any $s \in S$, $n = 1,2,...$ the inequality $$ V^{\pi_l}(s) \overset{!}{\le} \mathbb{E}_{\pi_u}\left[\sum_{k=0}^{n-1} \gamma^k r_{t+k} | s_t = s] + \gamma^n \mathbb{E}_{\pi_u}[V^{\pi_l}(s_{t+n}) | s_t = s\right] $$ holds. We will show this claim by induction over $n$.

	For the induction start, let $s \in S$ be arbitrary but fixed. We then see that, by our assumption, the definition of the value function $V^{\pi}$, and Proposition 2, 3., we have
	\[
		\begin{array}{rll}
			V^{\pi_l}(s) &	\le 	&	\mathbb{E}_{\pi_u}[Q^{\pi_l}(s_t,a_t) | s_t = s] \\
						&	=	&	\sum_a \pi_u(s,a) Q^{\pi_l}(s,a) \\
						&	=	&	\sum_a \pi_u(s,a) \mathbb{E}[r_t + \gamma V^{\pi_l}(s_{t+1}) | s_t = s, a_t = a]\\
						&	= 	&	\mathbb{E}_{\pi_u}[r_t + \gamma V^{\pi_l}(s_{t+1}) | s_t = s] \\
						&	=	&	\mathbb{E}_{\pi_u}[r_t | s_t = s] + \gamma \mathbb{E}_{\pi_u}[V^{\pi_l}(s_{t+1}) | s_t = s], \\
		\end{array}
	\]

	proving the claim for $n = 1$. Remember that the index $\pi_u$ denotes that any implicit intermediate action $a$ was taken according to $\pi_u$. 

	For the induction step, let us introduce some additional notation. Let $s_{t+k} \sim P_{s}^{k * \pi_u}$ denote the distribution for the state at $t+k$ given that the state at time $t$ was $s$ and the subsequent $k$ action(s) $a_t,\dots,a_{t+k-1}$ were chosen according to $\pi_u$. Then we can apply our expected value version of the initial assumption to see that

	\[
		\begin{array}{rll}
			V^{\pi_l}(s) & \overset{I.S.}{\le} & \mathbb{E}_{\pi_u}[\sum_{k=0}^{n-2} \gamma^k r_{t+k} | s_t = s] + \gamma^{n-1} \mathbb{E}_{\pi_u}[V^{\pi_l}(s_{t+n-1}) | s_t = s] \\
			& = & \mathbb{E}_{\pi_u}[\sum_{k=0}^{n-2} \gamma^k r_{t+k} | s_t = s] + \gamma^{n-1} \mathbb{E}_{s_{t+n-1} \sim P_s^{(n-1)*\pi_u}}[V^{\pi_l}(s_{t+n-1})] \\
			& \le & \mathbb{E}_{\pi_u}[\sum_{k=0}^{n-2} \gamma^k r_{t+k} | s_t = s] \\
			&  	& + \gamma^{n-1} \mathbb{E}_{(s_{t+n-1},a_{t+n-1}) \sim (P_s^{(n-1)*\pi_u},\pi_u(s_{t+n-1},\cdot)}[Q^{\pi_l}(s_{t+n-1},a_{t+n-1})] \\
		\end{array}
	\]

	Applying Corollary 2 with $P_{s',a'} = (P_s^{1 * \pi_u}, \pi_u(s',\cdot))$  to the last expression in the above sequence, we arrive at
	
	\[
		\begin{array}{rll}
		V^{\pi_l}(s) 	&	\le 	 & \mathbb{E}_{\pi_u}[\sum_{k=0}^{n-2} \gamma^k r_{t+k} | s_t = s] \\
					&		& + \gamma^{n-1} \mathbb{E}_{(s_{t+n-1},a_{t+n-1}) \sim (P_s^{(n-1)*\pi_u},\pi_u(s_{t+n-1},\cdot))}[r_{t+n-1} + \gamma V^{\pi_l}(s_{t+n})] \\
				 & = & \mathbb{E}_{\pi_u}[\sum_{k=0}^{n-2} \gamma^k r_{t+k} | s_t = s] \\
				& 	& + \gamma^{n-1} \mathbb{E}_{\pi_u}[r_{t+n-1} + \gamma V^{\pi_l}(s_{t+n}) | s_t = s] \\
				 & = & \mathbb{E}_{\pi_u}[\sum_{k=0}^{n-2} \gamma^k r_{t+k} | s_t = s]  + \mathbb{E}_{\pi_u}[\gamma^{n-1} r_{t+n-1} | s_t = s] \\
				&	& + \gamma^n \mathbb{E}_{\pi_u}[V^{\pi_l}(s_{t+n}) | s_t = s] \\
				& = & \mathbb{E}_{\pi_u}[\sum_{k=0}^{n-1} \gamma^k r_{t+k} | s_t = s] + \gamma^n \mathbb{E}_{\pi_u}[V^{\pi_l}(s_{t+n}) | s_t = s],
		\end{array}
	\]

	completing the induction step and proving the claim. Letting $n \rightarrow \infty$, we see that

	\[
		\begin{array}{rclcl}
			V^{\pi_l}(s) & \le & \mathbb{E}_{\pi_u}[\sum_{k=0}^{n} \gamma^k r_{t+k} | s_t = s] & + & \gamma^{n} \mathbb{E}_{\pi_u}[V^{\pi_l}(s_{t+n}) | s_t = s ] \\
						& \overset{n \rightarrow \infty}{\rightarrow } & V^{\pi_u}(s) & + & 0.
		\end{array}
	\]

	For the second term, we have implicitly used that $$0 \overset{\infty \leftarrow n}{\leftarrow} \min_{s \in S} V^{\pi_l}(s) \le \gamma^{n_a}\mathbb{E}_{\pi_u}[V^{\pi_l}(s_{t+n_a}) | s_t = s] \le \gamma^{n_a} \max_{s \in S} V^{\pi_l}(s) \overset{n \rightarrow \infty}{\rightarrow} 0.$$ The convergence claim regarding the first term was proven in Corollary \ref{cor_valSwitch}, \ref{eq_valSwitch}.

Informally speaking, taking an infinite number of actions according to $\pi_u$ before switching to $\pi_l$ essentially means simply following $\pi_u$ and generates, independent of the starting state $s$, an expected cumulative reward that is at least as high as the one generated by simply following $\pi_l$. This proves the theorem.

\end{proof}

The above theorem states that $\pi_u \ge_V \pi_l$ holds whenever $ \mathbb{E}_{a \sim \pi_u(s,\cdot )}[Q^{\pi_l}(s,a)] \ge V^{\pi_l}(s)$ for all $s \in S$, then $\pi_u \ge_V \pi_l.$ Can we tweak these assumptions to guarantee actual improvement over $\pi_l$, i.e. $\pi_u >_V \pi_l$?

\begin{thm}{(Improved policy improvement theorem)}
	Let $\pi_u, \pi_l$ be two different policies for a finite MDP.
	\begin{enumerate}
		\item If $$ \mathbb{E}_{a \sim \pi_u(s,\cdot )}[Q^{\pi_l}(s,a)] = V^{\pi_l}(s)$$ for all $s \in S$, then $$\pi_u =_V \pi_l.$$
		\item If $$ \mathbb{E}_{a \sim \pi_u(s,\cdot )}[Q^{\pi_l}(s,a)] \ge V^{\pi_l}(s)$$ for all $s \in S$, then $$\pi_u \ge_V \pi_l.$$
		\item If $$ \mathbb{E}_{a \sim \pi_u(s,\cdot )}[Q^{\pi_l}(s,a)] \ge V^{\pi_l}(s)$$ for all $s \in S$, then $$\pi_u >_V \pi_l$$ and there is at least one $s \in S$ such that $$ \mathbb{E}_{a \sim \pi_u(s,\cdot )}[Q^{\pi_l}(s,a)] > V^{\pi_l}(s),$$ then $$\pi_u >_V \pi_l.$$
	\end{enumerate}
\end{thm}

\begin{proof}
	Claim 2. is just the original Policy Improvement Theorem.

To see claim 1., simply replace the $\ge$ in the proof of the Policy Improvement Theorem with $=$. This equality persists through the induction step for all starting states $s$ and yields equality of the respective value functions $V^{\pi_u}$ and $V^{\pi_l}$.

To see the third claim, we again use the Policy Improvement Theorem to see that $\pi_u \ge_V \pi_l$. We now have to find at least one $s \in S$ satisfying $V^{\pi_u}(s) > V^{\pi_l}(s)$. Repeating the argument displayed in the proof of the Policy Comparison Theorem, we see that, applying it to the one state $s$ guaranteed by our starting assumption to achieve $\mathbb{E}_{a \sim \pi_u(s,\cdot )}[Q^{\pi_l}(s,a)] > V^{\pi_l}(s)$, the strict inequality persists throughout the induction step and really yields $V^{\pi_u}(s) > V^{\pi_l}(s)$. This shows $\pi_lu >_V \pi_l$ and completes the proof.
\end{proof}

We have now established some degree of comparability based on the policies' in question's $V$ and $Q$ functions. Before we can use this result to iteratively construct better and better policies, let us formalize the concept of a greedy policy.

\begin{defn}
	Let $\pi$ be a policy for a finite MDP and let $Q$ be some (possibly but not necessarily $\pi$'s) action-value function . For any state $s \in S$, denote by $$maxact^Q(s) := \{a \in A | Q(s,a) = \max Q(s,a)\},$$ the set of actions at which $Q(s,\cdot)$ achieves its maximum. We say that $\pi$ is \textit{Q}\textit{-greedy} if and only if for every $s \in S$ we have $$\sum_{a \in A} \pi(s,a) = \sum_{a \in maxact^Q(s)} \pi(s,a) = 1$$.
\end{defn}

If a policy is greedy w.r.t its own $Q$ function, we simply call it \textit{greedy}. In other words, a \textit{greedy} policy $\pi$ only chooses among the actions that maximise its own $Q^{\pi}$-function for the given state. The only thing that matters for these policies when making a decision in state $s$ on what to do next is whether that immediate action achieves a maximal 'immediate' pay-off of $\max_{a \in A} Q^{\pi}(s,a)$.

At first glance, such a policy might be somewhat short-sighted, seemingly ignoring potential future consquences of its immediate actions for the sake of instantaneous profit. However, the notion of the action-value function is to encode the future cumulative reward's expected value - in other words, it is a function that very much 'looks ahead' and considers future consequences; namely, all of them.

This means that a \textit{greedy} policy might not be as shortsighted, and therefore not such a bad thing, after all. Indeed, we will later see that the best policies are exactly the ones that are \textit{greedy}.

Let us therefore have a closer look at a greedy policy's value function.

\begin{cor}{(Greedy policy values)}
	Let $\pi_g$ be a \textit{greedy} policy for a finite MDP. Then $$V^{\pi_g} \equiv max_{a \in A} Q^{\pi_g}(\cdot,a).$$
\end{cor}

\begin{proof}
	We write for any fixed but arbitrary $s \in S$
	\[
		\begin{array}{rll}
			V^{\pi_g}(s) 	& = 	& \sum_a \pi_g(s,a) Q^{\pi_g}(s,a) \\
						& =	& \sum_{a \in maxact^{Q^{\pi_g}}} \pi_g(s,a) Q^{\pi_g}(s,a) \\
						& = 	& \max_{a \in A} Q^{\pi_g}(s,a).
		\end{array}
	\]
\end{proof}

In other words, a greedy policy's value function is just the maximum of its action-value function at the present state, taken over all possible actions. That is not surprising, seeing as maximising its own $Q$ function is what is driving a greedy policy as per definition.

The following result shows that any group of policies greedy w.r.t some $Q$ function are of the same quality. Unsurprisingly, the identical strategy of local maximisation does not lead to great deal of variability in policies' performances. It also shows that any policy that is not \textit{greedy} can be improved by making it \textit{greedy}.

\begin{lem}{(Greedy policy improvement)}
Let $\pi_g$ and $\pi_c$  be policies for finite MDP, and let $\pi_g$ be \textit{Q}$^{\pi_c}$\textit{-greedy}. If $\pi_c$ is \textit{Q}$^{\pi_c}$\textit{-greedy}, too, then $\pi_g =_V \pi_c$. Otherwise, $\pi_g >_V \pi_c$.
\end{lem}

\begin{proof}
	Let $s \in S$ be fixed but arbitrary. We write

	\[
		\begin{array}{rll}
			\mathbb{E}_{a \sim \pi_g(s,\cdot)}[Q^{\pi_c}(s,a)] 	& = & \sum_a \pi_g(s,a) Q^{\pi_c}(s,a) \\
													& = & \sum _{a \in maxact^{\pi_c}(s)} \pi_g(s,a) \max_{a \in A} Q^{\pi_c}(s,a) \\
													& = & \max_{a \in A} Q^{\pi_c}(s,a) \\
													& \ge & \sum_{a \in A} \pi_c(s,a) Q^{\pi_c}(s,a) \\
													& = & V^{\pi_c}(s).
		\end{array}
	\]

	If $\pi_c$ is \textit{Q}$^{\pi_c}$\textit{-greedy}, then we have equality in the above chain for all $s \in S$ and the Improved Policy Improvement Theorem, 1., implies $\pi_g =_V \pi_c$. If $\pi_c$ is not \textit{Q}$^{\pi_c}$\textit{-greedy}, then there is at least one $s \in S$ such that $>$ holds (and still $\ge$ for all other $s$). In that case the Improved Policy Improvement Theorem, 3., yields the desired claim.
\end{proof}

Inspecting the above proof we notice an important detail: Any deterministic policy that chooses an action from $maxact^{\pi_c}(s)$ given any state $s$ satisfies the condition of the corollary and thus is at least as good as (the potentially probabilistic) $\pi_c$. This means that for every probabilistic policy there is a deterministic policy that is at least as good, and it can be constructed explicitly by letting it choose any one action that maximises the probabilistic policy's action-value function.

\newpage

\section{Optimal policies and how to find them}

In this section we will use our concept of policy comparison to formalize and analyse the concept of a \textit{best} policy. We will investigate existence, uniqueness and characterizations of such policies. We will mainly build on the previous sections results, but will at a later point be forced to introduce some useful tools from functional analysis.

It is fair to say that most, if not all, of the subsequent results presented in this script deal with the analysis of optimal policies. In particular, we will formally answer the following questions:

\begin{itemize}
	\item Is there always a (unique) optimal policy for a finite MDP?
	\item Are there any characteristic traits that all optimal policies share, and if so, how can we make use of them to find these policies?
	\item Can we give a constructive way of finding or at least approximating these optimal policies?
\end{itemize}

With this road map in mind, let us start our journey by clarifying what we mean by an \textit{optimal} or \textit{best} policy.

\subsection{Existential crisis}

Let us begin this subsection by formalizing the concept of an optimal policy.

\begin{defn}{(Optimal policy)}\label{def_optPol}
	Let $\pi^*$ be a policy for a finite MDP. We call $\pi^*$ an \textit{optimal policy} if and only if we have $$\pi^* \ge_{V} \pi$$ for all policies $\pi \in \Pi$.
\end{defn}

In other words, a policy $\pi^*$ whose value function $V^{\pi^*}$ dominates the value functions $V^{\pi}$ of all other policies is optimal for the given finite MDP, and what we consider 'best'. This approach seems sensible, since optimality of a policy according to the above definition maximises the expected cumulative reward (that is what a policy's value function represents).

There's some good and some bad news. The good news is that for every finite MDP, there is at least on optimal policy $\pi^*$. The bad news is that that the proof for this existence seems kind of complicated - which is probably the reason it's omitted from pretty much every "online" source I have found so far. I'm currently reading a book by Bertseklis (?) on that subject, so hopefully that will point me in the right direction.

For now, we will just assume we have guaranteed the existence of an optimal policy $\pi^*$ for every finite MDP, and go from there. This brings us to the next subsection.

\subsection{Behavioural issues}

This subsection is dedicated to deriving some useful characteristics of the mysterious (as of yet) optimal policy of a given finite MDP. These results will turn out to be essential for the contruction of explicit algorithms aiming to converge on optimal policies. We start of with an easy corollary that states that, while there might be more than one optimal policy, they all share the same value and action value function. Since it is the value function that we use to rank and disambiguate policies from one another, the following statement essentially says that there is no need to tell one optimal policy apart from any other optimal policy, they are - literally for what it's worth - the same thing.

\begin{cor}{(It's all the same thing)}\label{cor_sameOptPol}
	Let $\pi_1, \pi_2$ be two optimal policies for a finite MDP. Then
	\begin{enumerate}
		\item \begin{equation}\label{eq_sameOptPolVal} V^{\pi_1} \equiv V^{\pi_2} \end{equation}
		\item \begin{equation}\label{eq_sameOptPolActVal} Q^{\pi_1} \equiv Q^{\pi_2} \end{equation}
	\end{enumerate}
\end{cor}

\begin{proof}
	Since both $\pi_1$ and $\pi_2$ are optimal policies, we have both
	\begin{equation}
		\pi_1 \ge_V \pi_2 \Leftrightarrow V^{\pi_1}(s) \ge V^{\pi_2}(s) \, \, \forall s \in S
	\end{equation}

	as well as

	\begin{equation}
		\pi_2 \le_V \pi_1 \Leftrightarrow V^{\pi_2}(s) \ge V^{\pi_1}(s) \, \, \forall s \in S,
	\end{equation}

	clearly implying the equality stated in \ref{eq_sameOptPolVal}. Claim \ref{eq_sameOptPolActVal} follows from \ref{eq_sameOptPolVal} and Corollary \ref{cor_sameVSameQ}.

\end{proof}

Knowing that all optimal policies share the same value and action value function, we can define the optimal versions of these functions independently of an attached optimal policy $\pi^*$.

\begin{defn}{(Optimal (action-)values)}\label{def_uniqueOptActVal}
	Let $\pi^*$ be any optimal policy for a given finite MDP. We call $V^* = V^{\pi^*}$ the optimal value function, and $Q^* = Q^{\pi^*}$ the optimal action-value function.
\end{defn}

Corollary \ref{cor_sameOptPol} guarantees the welldefinedness of the above terms. We go on to show a useful property of the optimal value function $V^*$.

\begin{lem}{(Different kind of optimum)}\label{cor_optValMaxS}
	For each $s \in S$ , the following hold:
	\begin{enumerate}
		\item \begin{equation}\label{eq_optValMax} V^*(s) = \max_{\pi \in \Pi} V^{\pi}(s) \end{equation}
		\item \begin{equation}\label{eq_optActValMax} Q^*(s,a) = \max_{\pi \in \Pi} Q^{\pi}(s,a) \end{equation}
	\end{enumerate}
\end{lem}

In other words, both the optimal value function and the optimal action-value function can be seen as the results of pointwise optimization over the state space $S$.

\begin{proof}
	We start with the first claim. Let $s \in S$ be fixed but arbitrary. By the very definition of $V^*$ as the value function of an optimal policy $\pi^*$ which satisfies \ref{}, we have

	\begin{equation}
		\max_{\pi \in \Pi} V^{\pi}(s) = \max_{\pi^* \in \Pi, \pi \text{optimal}} V^{\pi^*}(s).
	\end{equation}

	By Corollary \ref{cor_sameOptPol} we can \textit{arbitrarily} choose any optimal policy $\pi^*_1$ to obtain

	\begin{equation}
		\max_{\pi^* \in \Pi, \pi \text{optimal}} V^{\pi^*}(s) = V^{\pi^*_1}(s) = V^*(s),
	\end{equation}

	proving the identity \ref{eq_optValMax}. As for the second claim, let $(s,a) \in S \times A$ be arbitrary but fixed. Then

	\begin{equation}
		\begin{array}{rcl}
		\max_{\pi \in \Pi}Q^{\pi}(s,a) & \overset{Prop \ref{eq_optValMax}}{=} & \max_{\pi \in \Pi} \sum_{s'} P_{ss'}^a \left[ R_{ss'}^a + \gamma V^{\pi}(s') \right] \\
			& = & \sum_{s'} P_{ss'}^a R_{ss'}^a + \max_{\pi \in \Pi} \sum_{s'} P_{ss'}^a \gamma V^{\pi}(s').
		\end{array}
	\end{equation}

	where we have used that the term quantifying the expectation of the immediate reward $\mathbb{E}[r_t| s_t = s, a_t = a] = \sum_{s'} P_{ss'}^a R_{ss'}^a$ is independent of the particular policy $\pi$ if both $s_t$ and $a_t$ are fixed at $s$ and $a$, respectively. We see that

	\begin{equation}
		\begin{array}{rcl}
			& & \sum_{s'} P_{ss'}^a R_{ss'}^a + \max_{\pi \in \Pi} \sum_{s'} P_{ss'}^a \gamma V^{\pi}(s') \\
			& \le & \sum_{s'} P_{ss'}^a R_{ss'}^a + \sum_{s'} P_{ss'}^a \gamma  \max_{\pi \in \Pi} V^{\pi}(s') \\
			& \overset{\ref{eq_optValMax}}{=} & \sum_{s'} P_{ss'}^a R_{ss'}^a + \sum_{s'} P_{ss'}^a \gamma V^*(s') \\
			& = & \sum_{s'} P_{ss'}^a \left[ R_{ss'}^a + \gamma V^*(s') \right]. \\
		\end{array}
	\end{equation}

	Arbitrarily choosing an optimal policy $\pi^*$ we finally see that

	\begin{equation}
		\begin{array}{rcl}
			& & \sum_{s'} P_{ss'}^a \left[ R_{ss'}^a + \gamma V^*(s') \right] \\
			& \overset{Def \ref{def_uniqueOptActVal}}{=} & \sum_{s'} P_{ss'}^a \left[ R_{ss'}^a + \gamma V^{\pi^*}(s') \right] \\
			& \overset{Prop \ref{prop_qVRelShips}}{=} & Q^{\pi^*}(s,a) \\
			& \overset{Def \ref{def_uniqueOptActVal}}{=} & Q^*(s,a),
		\end{array}
	\end{equation}

	proving "$\ge$" in \ref{eq_optActValMax}. To see that "$\le$" holds, consider, again using some arbitrary optimal policy $\pi^*$, that, since the optimal policy $\pi^*$, too, is an element of the set of \textit{all} policies $\Pi$, we also have

	\begin{equation}
		Q^*(s,a) \overset{Def \ref{def_uniqueOptActVal}}{=} Q^{\pi^*}(s,a) \le \max_{\pi \in \Pi} Q^{\pi}(s,a).
	\end{equation}

	Since $(s,a) \in S \times A$ was arbitrary, $\ge$ and $\le$ together yield \ref{eq_optActValMax}.
\end{proof}

\end{document}
